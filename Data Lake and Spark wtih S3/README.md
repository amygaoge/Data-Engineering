# Sparkify AWS Cloud Data Lake and Spark ETL

## Context
A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like a data engineer to build an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.


## ETL Pipeline
This project scales up the data analysis process through the use of a data lake and Spark, in order to further optimize queries on song play analysis.Spark achives high performance when processing analytic workloads on big data sets. Using in-memory computing, parallel processing and lazy evaluation, Spark supports fast query, analyze, and transform data at scale across a cluster with multiple machines. Data lake supports different data format and structe and supports schema-on-read, which provides the Sparkify analytics team a flexible architecture to do ad-hoc data exploration and advanced analytics.

The ETL pipeline extracts data from S3, processes the data using Spark and transforms the data into star schema analytics tables, and finally stores the tables back into S3 buckets with parquet formatting and efficient partitioning.


## Database Schema
Star Schema is used on analytics tables to give users the ability to perform simple query with less joins and fast aggregations. The Fact Table records in log data associated with song plays. Using this table, the company can relate and analyze four dimensions users, songs, artists and time.

Fact Table: songplays
Dimension Tables: users, songs, artists and time.


## Project file structure
* **[etl_testing.ipynb]**: Python notebook to test etl logic
* **[dwh.cfg]**: Config file which contains AWS credentials
* **[etl.py]**: Python script to extracts raw data from S3, transforms data into a set of dimensional analytics tables using Spark, and stores the tables back into S3 buckets


### Dataset
The Song dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song.

The Log dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

- **Song datasets**: all json files are nested in subdirectories under */data/song_data*. A sample of this files is:

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

- **Log datasets**: all json files are nested in subdirectories under */data/log_data*. A sample of a single row of each files is:

```
{"artist":"Slipknot","auth":"Logged In","firstName":"Aiden","gender":"M","itemInSession":0,"lastName":"Ramirez","length":192.57424,"level":"paid","location":"New York-Newark-Jersey City, NY-NJ-PA","method":"PUT","page":"NextSong","registration":1540283578796.0,"sessionId":19,"song":"Opium Of The People (Album Version)","status":200,"ts":1541639510796,"userAgent":"\"Mozilla\/5.0 (Windows NT 6.1) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"20"}
```